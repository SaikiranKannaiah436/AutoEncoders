{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "batch_size = 32\n",
    "latent_size = 20 # z dim\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    x = Variable(x)\n",
    "    if use_cuda:\n",
    "        x = x.cuda()\n",
    "    return x\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    bs = recon_x.size(0)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 28*28), size_average=False)\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (BCE + KLD) / bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, feature_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size, 400)\n",
    "        self.fc21 = nn.Linear(400, latent_size)\n",
    "        self.fc22 = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x): # Q(z|x)\n",
    "        '''\n",
    "        x: (bs, feature_size)\n",
    "        '''\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        z_mu = self.fc21(h1)\n",
    "        z_var = self.fc22(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std) + mu\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z): # P(x|z)\n",
    "        '''\n",
    "        z: (bs, latent_size)\n",
    "        '''\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 28*28))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data = to_var(data)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (data, labels) in enumerate(test_loader):\n",
    "        data = to_var(data)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                  recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                     'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE(28*28, latent_size)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 17.218344\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 4.081539\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 3.871977\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 3.817138\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3.761353\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 3.529510\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 3.370446\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.897512\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 3.209020\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 3.244155\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 3.477097\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 3.319956\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 3.245243\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 3.372927\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 3.549539\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 3.354701\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 3.576604\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 3.483650\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 3.496179\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 3.354993\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 3.536632\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 3.391379\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 3.271328\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 3.383426\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 3.537288\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 3.192325\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 3.459355\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 3.150836\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 3.526077\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 3.358228\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 3.391103\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 3.297000\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 3.307764\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 3.411191\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 3.315590\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 3.318712\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition labels: 8 --> 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAABJCAYAAADR/0nqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHKEgCIIgCJqW/wHplX6tgVAbQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64a80da9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see a transition from labels[0] to labels[1]\n",
    "model.eval()\n",
    "for i, (data, labels) in enumerate(train_loader):\n",
    "    data = to_var(data)\n",
    "    mu, logvar = model.encode(data.view(-1, 28*28))\n",
    "#     print(mu.size(), mu[:3], logvar[:3])\n",
    "    z = model.reparametrize(mu, logvar)\n",
    "    print('Transition labels:', labels[0], '-->', labels[1])\n",
    "    z_cont = to_var(torch.zeros(10, latent_size))\n",
    "    for i in range(10):\n",
    "        t = 1.0 - i/9\n",
    "        z_cont[i] = t * z[0] + (1-t) * z[1]\n",
    "    samples = model.decode(z_cont).data.cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = gridspec.GridSpec(10, 10)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
